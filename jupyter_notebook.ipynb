{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Do I write like Shakespeare?\n",
    "This notebook was written as an attempt to address the following problem. How do you know when what someone is writing is really from them, as opposed to copying someone else? $$ $$\n",
    "One way to tell is to look at their word choice. Each person has a pattern of word choice and favorite expressions. My friend James likes to say “It beats a sharp stick in the eye!” when something is not too bad, my father-in-law: “Gimme a break!” when he thinks something is absurd. He also quotes Shakespeare. $$ $$\n",
    "So, if we have 2 texts, say, one from Shakespeare and another from a lay person, by taking a sample of 20 words from what they say, we should be able to tell if they are trying to copy, or quote, Shakespeare or if it is their own speech production.\n",
    "In this notebook, I use a technique called “Bag Of Words”, in which a particular text, or a particular sample, is a vector of word counts. $$ $$\n",
    "So the first step is to find out the set of unique words, the vocabulary, the two texts use, and how many such unique words, called tokens, there are. $$ $$\n",
    "Doing so requires the use of a tokenizer program. I use the keras.preprocessing.text one. $$ $$\n",
    "In this notebook I also use Tensorflow. Keras uses Tensorflow as backend. $$ $$\n",
    "If you do not have Tensorflow, there is a 10 dollar and easy way to do so, as long as you have an Android phone. The app is called Pydroid 3. Sorry Apple users, it is not available in the Apple store. You can install Tensorflow on your computer, but it tends to interfere with other programs, whereas the phone install gives you a binary that is compatible with other libraries you may want to install in the future. All the Python libraries you can dream of are available. $$ $$\n",
    "For most of you, I included the tokenized versions of the two texts as files loadable with numpy, so you do not have to worry about the keras tokenizer. The section with the neural network can just be skipped if you do not have Tensorflow. $$ $$\n",
    "#### This notebook is available at: https://github.com/pzirnhel/NewbyRepository.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer()\n",
    "\n",
    "data = open('tmp/CoriolanusAndOctavioShortNoPunctuationNoApostrophe.txt').read()\n",
    "\n",
    "corpus = data.lower().split(\"\\n\")\n",
    "\n",
    "\n",
    "tokenizer.fit_on_texts(corpus)\n",
    "total_words = len(tokenizer.word_index) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "734\n"
     ]
    }
   ],
   "source": [
    "print(total_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are 734 different words in the two texts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The word index is a dictionary that gives the word token number of the word\n",
    "# This cell builds the opposite: idx_to_word gives the word knowing the index. It is useful for printing.\n",
    "word_index = tokenizer.word_index\n",
    "idx_to_word=[]\n",
    "for word in word_index:\n",
    "    idx_to_word.append(word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the next cell, we make a long string of all the words, ranked by frequency, most frequent first, there are in the two texts. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the to you it of a and citizen we in first for is with what not was that i he us all on this our one have be but s are they so would if as menenius she his or more did name units him these your belly speak know at an could do when image list representation t second them well say can must other tell like only then about each hear me let ll against why who will by answer long octavio her find want sutsbencuns ai input layer good poor were their give which even o now strong sir most up there off body look hornada just today how patel geoff professor line examples any resolved rather than people done patricians think too very hath mother cannot way need my where go had shall already care may make daily d see made from such chapter ocean perhaps left another last take going put two b get right use network given much its training goes before proceed further famish caius marcius no talking away citizens wholesome revenge gods has country content proud unto though soft please help here come honest rest work hand matter arms friends yourselves dearth state ne er yet rich restrain love accused tale heard little time members thus common kind smile came small found basket baby discovered police lead easy morning instructor breeze straight got run down maybe does introduction familiar audience algorithm without dots between black white numbers call vector value neural feature turns lines trying features batch suppose saying sees portion die chief enemy kill corn own price verdict word accounted authority surfeits relieve yield superfluity while might guess relieved humanely dear leanness afflicts object misery inventory particularise abundance sufferance gain pikes ere become rakes hunger bread thirst especially dog commonalty consider services report fort pays himself being nay maliciously famously end conscienced men partly till altitude virtue nature account vice covetous barren accusations faults surplus tire repetition shouts side city risen stay prating capitol comes worthy agrippa always loved enough countrymen bats clubs pray business unknown senate inkling fortnight intend show em deeds suitors breaths masters mine neighbours undo undone charitable wants suffering strike heaven staves lift roman whose course takes cracking ten thousand curbs link asunder ever appear impediment knees alack transported calamity thither attends slander helms fathers curse enemies true indeed cared suffer store houses crammed grain edicts usury support usurers repeal act established provide piercing statutes chain wars eat bear either confess wondrous malicious folly pretty since serves purpose venture stale fob disgrace deliver rebell gulf remain midst idle unactive still cupboarding viand never bearing labour instruments devise instruct walk feel mutually participate minister appetite affection whole lungs tauntingly replied discontented mutinous parts envied receipt fitly malign senators kingly crowned head vigilant eye counsellor heart arm soldier steed leg tongue trumpeter muniments petty helps fabric fore fellow speaks should cormorant sink former agents complain bestow patience awhile ye re multiple life childhood born lima peru birth been avoid pregnancy raise bench parque de amor park strip grass miraflores tops cliffs facing pacific hoped jogger women practicing thai chi crack dawn tag strange eighth child intimately personal knew leave father hers obvious cue social worker interrogate potential relatives named necessarily investigation abandoned infant nowhere inquiry stuck paragliding setting 10 30 slowly dissipating fog smooth getting soar buildings beautiful flight sunny autumn day flying shouted brunette walking towards yes answered lucky said softly level ve nice wind coming fly high signed waiver papers nodded few times showed harness died moment shouting hungry asked sound cat mitsuko batchi three basheer interview pr musical welcome online podcast hessian honor prolific contributors field artificial intelligence opportunity thank figure hardly co inventors backpropagation powers pioneers language models directed wide open project developed ground grinding ginormous gorgeous prediction tantalizer otherwise known 4gpt attempting paraphrase delighted ideas due milestones interesting billions dollars pouring into because started published representations start grid pixels next degree grayness zero 1 dot after represent many sometimes neurons hence term particular tries shape place stays less depending finds resembles easier break under itself keep new becomes sounds back 1980s unit crafted automatically train desired output matching using series model testing different ones used although same build learned match handwritten digit wrote actual similar forgive interrupting stochastic gradient descent engine uses basis better ahead paraphrasing procedure networks guys publications wanted layers map turn also both portions \n"
     ]
    }
   ],
   "source": [
    "all_words=\"\"\n",
    "for idx in range(len(idx_to_word)):\n",
    "    all_words=all_words+idx_to_word[idx]+\" \"\n",
    "print(all_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "283\n"
     ]
    }
   ],
   "source": [
    "# This gives the number of lines the two texts put together has\n",
    "lines=[]\n",
    "for line in corpus:\n",
    "    lines.append(line)\n",
    "print(len(lines))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "first citizen\n"
     ]
    }
   ],
   "source": [
    "# This is the first line of \"The Tragedy of Coriolanus\"\n",
    "print(lines[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The purpose of the next cell is to identify the last line pertaining to Shakespeare in the two texts, in order to make two separate lists, one being the list of tokens of the Shakespeare text: “coriolanus_tokenized”, the other one my prose: “octavio_tokenized”."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "1\n",
      "37\n",
      "menenius\n",
      "4\n",
      "19\n",
      "i\n",
      "80\n",
      "will\n",
      "67\n",
      "tell\n",
      "3\n",
      "you\n",
      "11\n",
      "35\n",
      "if\n",
      "3\n",
      "you\n",
      "76\n",
      "ll\n",
      "487\n",
      "bestow\n",
      "6\n",
      "a\n",
      "219\n",
      "small\n",
      "5\n",
      "of\n",
      "15\n",
      "what\n",
      "3\n",
      "you\n",
      "27\n",
      "have\n",
      "211\n",
      "little\n",
      "9\n",
      "488\n",
      "patience\n",
      "489\n",
      "awhile\n",
      "3\n",
      "you\n",
      "76\n",
      "ll\n",
      "73\n",
      "hear\n",
      "1\n",
      "the\n",
      "48\n",
      "belly\n",
      "30\n",
      "s\n",
      "82\n",
      "answer\n",
      "1\n",
      "2\n",
      "11\n",
      "first\n",
      "8\n",
      "citizen\n",
      "5\n",
      "490\n",
      "ye\n",
      "491\n",
      "re\n",
      "83\n",
      "long\n",
      "71\n",
      "about\n",
      "4\n",
      "it\n",
      "1\n",
      "1\n",
      "37\n",
      "menenius\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "for current_line_nb in range(190,201): #201 is the first line of the Octavio file\n",
    "    current_line=lines[current_line_nb].lower().split(\" \")\n",
    "    print(len(current_line))\n",
    "    for idx in range(len(current_line)):\n",
    "        if current_line[idx] is not '':\n",
    "            print(word_index[current_line[idx]])\n",
    "            print(current_line[idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "coriolanus_tokenized=[]\n",
    "for current_line_nb in range(201): #201 is the first line of the Octavio file\n",
    "    current_line=lines[current_line_nb].lower().split(\" \")\n",
    "    #print(len(current_line))\n",
    "    for idx in range(len(current_line)):\n",
    "        if current_line[idx] is not '':\n",
    "            coriolanus_tokenized.append(word_index[current_line[idx]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[11, 8, 168, 9, 169, 118, 170, 73, 74, 49, 22, 49, 49, 11, 8, 3, 31, 22, 119, 120, 2, 260, 121, 2, 171, 22, 119, 119, 11, 8, 11, 3, 50, 172, 173, 13, 261, 262, 2, 1, 122, 22, 9, 50, 59, 9, 50, 59, 11, 8, 75, 21, 263, 45, 7, 9, 76, 27, 264, 51, 25, 265, 266, 13, 59, 6, 267, 22, 174, 41, 175, 23, 59, 75, 4, 28, 123, 176, 176, 60, 8, 26, 268, 92, 177, 11, 8, 9, 31, 269, 93, 177, 1, 124, 92, 15, 270, 271, 23, 34, 272, 21, 35, 32, 34, 273, 21, 29, 1, 274, 275, 4, 94, 178, 9, 276, 277, 32, 278, 21, 279, 29, 32, 125, 9, 31, 126, 280, 1, 281, 18, 282, 21, 1, 283, 5, 25, 284, 13, 36, 52, 285, 2, 286, 95, 287, 25, 288, 13, 6, 289, 2, 61, 75, 21, 179, 24, 14, 25, 290, 291, 9, 292, 293, 12, 1, 180, 50, 19, 49, 24, 10, 294, 12, 295, 16, 10, 296, 12, 179, 60, 8, 34, 3, 169, 297, 77, 172, 173, 22, 77, 45, 11, 20, 30, 6, 127, 298, 2, 1, 299, 60, 8, 300, 3, 15, 301, 20, 181, 123, 12, 39, 182, 11, 8, 127, 62, 7, 53, 28, 183, 2, 96, 45, 92, 302, 303, 29, 18, 20, 304, 305, 14, 306, 184, 60, 8, 307, 29, 49, 16, 308, 11, 8, 19, 63, 185, 3, 15, 20, 128, 123, 309, 20, 42, 4, 2, 18, 310, 186, 187, 311, 312, 64, 28, 183, 2, 63, 4, 17, 12, 39, 182, 20, 42, 4, 2, 188, 39, 129, 7, 2, 28, 313, 184, 97, 20, 13, 98, 314, 1, 315, 5, 39, 316, 60, 8, 15, 20, 130, 189, 10, 39, 317, 3, 318, 6, 319, 10, 45, 3, 65, 10, 174, 131, 63, 20, 13, 320, 11, 8, 35, 19, 65, 16, 19, 132, 16, 28, 321, 5, 322, 20, 128, 323, 14, 324, 2, 325, 10, 326, 15, 327, 31, 46, 1, 66, 328, 99, 1, 329, 13, 330, 78, 331, 9, 332, 190, 2, 1, 333, 22, 191, 191, 11, 8, 187, 79, 334, 190, 60, 8, 335, 37, 336, 26, 18, 128, 337, 338, 1, 122, 11, 8, 20, 30, 26, 192, 339, 34, 22, 1, 193, 94, 33, 37, 15, 194, 30, 133, 340, 10, 195, 134, 135, 3, 14, 341, 7, 342, 1, 196, 49, 19, 343, 3, 11, 8, 25, 344, 13, 16, 345, 2, 1, 346, 32, 27, 136, 347, 24, 348, 15, 9, 349, 2, 54, 97, 100, 9, 76, 350, 351, 10, 352, 32, 63, 93, 353, 27, 101, 354, 32, 137, 50, 9, 27, 101, 197, 126, 37, 78, 355, 133, 92, 198, 356, 192, 357, 80, 3, 358, 199, 11, 8, 9, 130, 102, 9, 31, 359, 138, 37, 19, 67, 3, 198, 103, 360, 139, 27, 1, 124, 5, 3, 12, 47, 361, 47, 362, 10, 24, 200, 3, 140, 36, 62, 363, 51, 1, 364, 14, 47, 365, 36, 366, 61, 77, 1, 367, 201, 368, 369, 80, 23, 1, 131, 4, 370, 371, 372, 373, 374, 5, 41, 101, 375, 376, 121, 64, 377, 378, 10, 47, 379, 12, 1, 200, 1, 180, 16, 1, 124, 141, 4, 7, 47, 380, 2, 61, 16, 197, 65, 189, 381, 3, 31, 382, 81, 383, 384, 134, 41, 385, 3, 7, 3, 386, 1, 387, 99, 1, 201, 79, 139, 12, 3, 68, 388, 55, 3, 389, 61, 36, 390, 11, 8, 139, 12, 21, 391, 392, 32, 202, 203, 393, 12, 21, 204, 394, 21, 2, 171, 7, 95, 395, 396, 397, 14, 398, 141, 399, 12, 400, 2, 401, 402, 403, 142, 118, 178, 404, 405, 77, 1, 205, 7, 406, 41, 407, 408, 142, 2, 409, 104, 7, 206, 1, 93, 35, 1, 410, 411, 21, 16, 104, 32, 80, 7, 105, 30, 22, 1, 207, 32, 412, 21, 37, 413, 3, 65, 414, 199, 415, 416, 40, 28, 208, 5, 417, 19, 137, 67, 3, 6, 418, 209, 4, 140, 28, 3, 27, 210, 4, 29, 419, 4, 420, 133, 421, 19, 80, 422, 2, 423, 59, 6, 211, 41, 11, 8, 62, 19, 76, 73, 4, 102, 204, 3, 65, 16, 125, 2, 424, 106, 25, 425, 14, 6, 209, 29, 52, 59, 188, 3, 426, 37, 105, 17, 6, 212, 55, 22, 1, 107, 30, 213, 427, 143, 77, 1, 48, 214, 208, 4, 18, 69, 68, 6, 428, 4, 42, 429, 19, 1, 430, 99, 1, 107, 431, 7, 432, 433, 434, 1, 435, 436, 437, 68, 438, 14, 1, 193, 134, 1, 66, 439, 42, 144, 7, 73, 440, 441, 442, 443, 7, 444, 445, 42, 446, 185, 1, 447, 7, 448, 215, 5, 1, 449, 107, 1, 48, 82, 143, 11, 8, 62, 102, 15, 82, 145, 1, 48, 37, 102, 19, 137, 67, 3, 14, 6, 216, 5, 217, 97, 202, 203, 218, 146, 1, 450, 29, 98, 214, 12, 108, 3, 19, 140, 141, 1, 48, 217, 36, 62, 36, 49, 4, 451, 452, 2, 1, 453, 213, 1, 454, 455, 18, 456, 39, 457, 98, 33, 103, 458, 36, 3, 459, 25, 460, 12, 18, 32, 31, 16, 147, 36, 3, 11, 8, 47, 48, 30, 82, 15, 1, 461, 462, 463, 1, 464, 465, 1, 466, 467, 1, 468, 25, 469, 25, 470, 1, 471, 1, 472, 25, 473, 14, 66, 474, 7, 475, 476, 10, 24, 25, 477, 35, 18, 32, 37, 15, 70, 478, 74, 24, 479, 480, 15, 70, 15, 70, 11, 8, 481, 81, 1, 482, 48, 28, 206, 143, 79, 13, 1, 483, 99, 1, 107, 37, 62, 15, 70, 11, 8, 1, 484, 485, 35, 32, 42, 486, 15, 53, 1, 48, 82, 37, 19, 80, 67, 3, 35, 3, 76, 487, 6, 219, 5, 15, 3, 27, 211, 488, 489, 3, 76, 73, 1, 48, 30, 82, 11, 8, 490, 491, 83, 71, 4, 37]\n"
     ]
    }
   ],
   "source": [
    "print(coriolanus_tokenized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [],
   "source": [
    "octavio_tokenized=[]\n",
    "for current_line_nb in range(201,283): #201 is the first line of the Octavio file\n",
    "    current_line=lines[current_line_nb].lower().split(\" \")\n",
    "    #print(len(current_line))\n",
    "    for idx in range(len(current_line)):\n",
    "        if current_line[idx] is not '':\n",
    "            octavio_tokenized.append(word_index[current_line[idx]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 492, 493, 5, 84, 109, 148, 26, 84, 30, 494, 84, 17, 495, 10, 496, 497, 39, 498, 129, 65, 27, 499, 126, 93, 2, 500, 501, 7, 2, 502, 45, 20, 17, 220, 10, 6, 221, 23, 6, 503, 51, 504, 505, 506, 1, 507, 5, 207, 1, 508, 5, 509, 10, 510, 18, 511, 1, 512, 513, 1, 514, 149, 150, 39, 129, 515, 85, 222, 34, 28, 223, 81, 6, 516, 40, 26, 5, 46, 205, 517, 518, 519, 520, 51, 1, 521, 5, 522, 38, 136, 110, 151, 1, 221, 14, 6, 523, 23, 4, 84, 109, 15, 6, 524, 43, 84, 150, 24, 17, 85, 525, 526, 7, 38, 53, 16, 86, 152, 41, 527, 528, 43, 40, 150, 38, 529, 38, 34, 27, 2, 75, 45, 135, 7, 78, 42, 38, 530, 6, 153, 43, 78, 110, 26, 153, 43, 17, 4, 39, 531, 30, 43, 7, 38, 34, 16, 96, 532, 6, 43, 13, 147, 52, 533, 534, 1, 535, 536, 40, 1, 224, 34, 537, 538, 539, 5, 122, 540, 109, 4, 34, 541, 225, 2, 85, 29, 4, 17, 16, 226, 1, 224, 42, 108, 1, 43, 104, 109, 13, 16, 215, 1, 542, 53, 16, 86, 79, 53, 27, 543, 1, 544, 24, 153, 43, 225, 545, 1, 546, 42, 154, 6, 83, 212, 186, 33, 1, 43, 547, 1, 227, 20, 17, 223, 1, 548, 228, 17, 549, 104, 4, 17, 71, 550, 551, 1, 149, 229, 17, 552, 553, 1, 227, 554, 4, 17, 555, 7, 138, 556, 101, 230, 10, 110, 15, 3, 132, 2, 557, 1, 558, 4, 17, 155, 2, 28, 6, 559, 560, 23, 6, 561, 562, 563, 31, 3, 564, 14, 21, 111, 20, 565, 2, 6, 566, 79, 17, 567, 568, 45, 569, 38, 570, 3, 31, 127, 571, 111, 20, 572, 41, 573, 36, 38, 17, 100, 51, 39, 574, 9, 575, 231, 6, 576, 149, 577, 578, 230, 10, 9, 31, 155, 2, 579, 580, 111, 27, 3, 581, 1, 582, 583, 38, 584, 54, 3, 50, 112, 2, 232, 19, 87, 3, 2, 96, 4, 22, 3, 231, 20, 145, 85, 232, 6, 585, 586, 7, 70, 587, 85, 112, 2, 156, 23, 1, 588, 1, 229, 589, 233, 12, 6, 590, 18, 30, 55, 38, 210, 45, 1, 222, 234, 4, 17, 1, 591, 40, 234, 20, 17, 592, 38, 593, 1, 228, 54, 3, 50, 15, 24, 13, 4, 235, 16, 594, 68, 6, 595, 75, 74, 108, 148, 157, 596, 597, 148, 598, 599, 113, 30, 600, 14, 601, 114, 88, 602, 236, 158, 113, 603, 2, 25, 604, 605, 1, 606, 4, 13, 52, 607, 2, 27, 26, 5, 1, 103, 608, 609, 2, 1, 610, 5, 611, 612, 14, 21, 111, 33, 19, 87, 2, 11, 154, 24, 613, 2, 614, 3, 115, 88, 3, 31, 6, 237, 615, 2, 25, 238, 7, 616, 132, 52, 236, 3, 94, 26, 5, 1, 617, 618, 5, 1, 619, 239, 18, 100, 620, 103, 5, 89, 26, 5, 1, 621, 5, 622, 623, 7, 624, 1, 625, 626, 89, 627, 18, 628, 1, 629, 630, 631, 632, 633, 634, 635, 636, 36, 637, 33, 120, 121, 74, 638, 2, 639, 15, 3, 53, 67, 21, 19, 34, 28, 640, 2, 73, 146, 3, 112, 22, 46, 641, 218, 71, 33, 240, 170, 642, 115, 67, 21, 71, 46, 89, 643, 114, 88, 4, 13, 644, 2, 144, 1, 645, 5, 646, 100, 647, 648, 89, 649, 55, 9, 650, 9, 53, 16, 159, 651, 4, 30, 22, 71, 652, 63, 3, 653, 14, 52, 56, 6, 654, 5, 655, 46, 219, 241, 160, 656, 2, 72, 66, 18, 64, 154, 118, 657, 5, 658, 242, 243, 7, 244, 659, 13, 12, 243, 660, 12, 244, 33, 24, 56, 13, 68, 6, 83, 57, 5, 245, 9, 246, 4, 6, 247, 35, 3, 57, 72, 661, 248, 151, 2, 160, 12, 72, 116, 7, 156, 72, 116, 26, 662, 1, 66, 3, 87, 2, 663, 24, 90, 56, 33, 3, 161, 664, 44, 9, 665, 246, 61, 666, 667, 1, 668, 249, 162, 72, 181, 6, 669, 250, 4, 670, 2, 86, 10, 1, 90, 63, 6, 116, 5, 6, 163, 671, 51, 6, 163, 672, 10, 1, 56, 35, 4, 220, 4, 4, 251, 23, 35, 16, 4, 673, 106, 40, 120, 4, 251, 23, 41, 40, 674, 675, 23, 112, 164, 15, 4, 676, 677, 165, 250, 33, 35, 3, 108, 51, 47, 83, 57, 5, 44, 3, 159, 152, 83, 57, 5, 241, 23, 40, 106, 40, 10, 242, 6, 58, 5, 1, 56, 4, 13, 678, 2, 125, 5, 4, 35, 3, 679, 233, 1, 57, 10, 252, 7, 156, 1, 252, 680, 72, 66, 29, 4, 235, 16, 196, 1, 57, 5, 245, 1, 247, 681, 13, 1, 58, 3, 64, 682, 155, 68, 24, 161, 1, 58, 145, 81, 47, 57, 5, 44, 36, 52, 90, 2, 152, 91, 5, 44, 253, 2, 86, 95, 254, 10, 1, 58, 24, 683, 91, 5, 44, 684, 70, 6, 58, 5, 1, 58, 5, 1, 90, 56, 18, 685, 226, 29, 686, 10, 1, 687, 22, 46, 688, 254, 136, 2, 28, 195, 689, 9, 94, 253, 2, 86, 6, 131, 2, 690, 691, 6, 249, 162, 2, 96, 21, 6, 692, 693, 58, 163, 165, 694, 90, 695, 6, 696, 5, 697, 117, 12, 166, 6, 255, 5, 117, 12, 698, 9, 27, 2, 161, 117, 699, 146, 1, 700, 701, 12, 166, 702, 5, 1, 703, 216, 9, 704, 147, 6, 162, 18, 705, 2, 706, 1, 707, 56, 5, 6, 708, 3, 110, 709, 2, 165, 710, 248, 81, 166, 4, 168, 23, 6, 255, 5, 711, 117, 158, 113, 712, 74, 115, 12, 713, 19, 256, 3, 31, 175, 71, 1, 714, 715, 716, 239, 1, 717, 5, 89, 25, 238, 718, 23, 6, 142, 719, 10, 95, 194, 7, 97, 13, 33, 237, 2, 61, 18, 4, 167, 240, 257, 114, 88, 18, 30, 78, 4, 167, 98, 167, 720, 257, 4, 158, 113, 135, 721, 115, 33, 164, 12, 16, 722, 114, 88, 105, 17, 138, 6, 723, 2, 54, 4, 14, 724, 14, 69, 26, 91, 5, 44, 46, 725, 34, 159, 22, 1, 726, 29, 9, 727, 2, 54, 4, 14, 157, 40, 41, 728, 3, 144, 105, 13, 69, 33, 164, 3, 64, 729, 14, 69, 26, 91, 5, 44, 256, 3, 87, 26, 5, 46, 44, 2, 730, 23, 55, 4, 258, 69, 1, 160, 259, 5, 6, 116, 7, 731, 23, 55, 4, 13, 69, 1, 151, 259, 29, 3, 87, 4, 2, 28, 106, 55, 4, 258, 732, 733, 3, 130, 54, 24, 14, 26, 91, 5, 44, 29, 3, 64, 54, 4, 14, 157]\n"
     ]
    }
   ],
   "source": [
    "print(octavio_tokenized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1010 1145\n"
     ]
    }
   ],
   "source": [
    "print(len(coriolanus_tokenized),len(octavio_tokenized))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first text, the first five pages of \"The Tragedy of Coriolanus\" written by Skakespeare is a list of 1010 tokens. $$ $$\n",
    "The second, the first five pages of a would be novel called \"The Multiple Lives of Octavio Hornada\" written by me is a list of 1145 tokens. Close enough. $$ $$\n",
    "In the next cell, I save these lists and loads them back. I commented all of it. If you do not have the keras tokenizer available, you can simply un-comment the two load instructions, to get the lists."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [],
   "source": [
    "#np.save('tmp/coriolanus_tokenized',np.array(coriolanus_tokenized))\n",
    "#np.save('tmp/octavio_tokenized',np.array(octavio_tokenized))\n",
    "#coriolanus_tokenized=list(np.load('tmp/coriolanus_tokenized.npy'))\n",
    "#octavio_todenized=list(np.load('tmp/octavio_tokenized.npy'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Each array is shuffled, in order to extract random samples from them\n",
    "coriolanus_shuffled=np.array(coriolanus_tokenized)\n",
    "np.random.shuffle(coriolanus_shuffled)\n",
    "octavio_shuffled=np.array(octavio_tokenized)\n",
    "np.random.shuffle(octavio_shuffled)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next step is to extract 50 “lists” of 20 tokens from each numpy array: that will give us a total of 100 samples. $$ $$\n",
    "They are not really lists, simply rows of two 50 by 20 numpy arrays. $$ $$\n",
    "50 times 20 = 1000. We only extract 1000 tokens from the two texts and discard the remainder. Because we shuffled the tokenized texts, the samples are random. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n"
     ]
    }
   ],
   "source": [
    "input_dim=20\n",
    "n_samples=2000//input_dim\n",
    "X_coriolanus_idx=np.reshape(coriolanus_shuffled[:1000],[n_samples//2,input_dim])\n",
    "X_octavio_idx=np.reshape(octavio_shuffled[:1000],[n_samples//2,input_dim])\n",
    "print(n_samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now it is time to convert the samples from lists of tokens to vectors of word counts. $$ $$\n",
    "It turned out the most common five words were a source of noise in this classification task. $$ $$\n",
    "So the vectors do not include these five features: that is if the sample contains one of these frequent words, we do not count it. This creates vectors with 729 dimensions instead of 734."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [],
   "source": [
    "freq_words=7\n",
    "last_words=total_words-freq_words\n",
    "X_coriolanus=np.zeros([n_samples//2,last_words])\n",
    "X_octavio=np.zeros([n_samples//2,last_words])\n",
    "for sample in range(n_samples//2):\n",
    "    for j in range(input_dim):\n",
    "        word_idx = X_coriolanus_idx[sample,j]-freq_words\n",
    "        if word_idx>=0:\n",
    "            X_coriolanus[sample,word_idx]+=1.0\n",
    "        word_idx = X_octavio_idx[sample,j]-freq_words\n",
    "        if word_idx>=0:\n",
    "            X_octavio[sample,word_idx]+=1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0.\n",
      " 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 1. 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "# This prints an example of word count vectors: print(X_coriolanus[0,:])\n",
    "# The vector is very sparse: lots of zeros\n",
    "print(X_octavio[0,:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next six cells are about organizing the data into training and test sets along with their corresponding labels: 1 for Shakespeare, 0 for not Shakespeare. $$ $$\n",
    "First, we get a random ordering of numbers from 0 to 99. We will use this ordering to shuffle the data the same way we shuffle the labels, so each sample gets its correct corresponding label. $$ $$\n",
    "Second we get a numpy array of 50 ones and 50 zeros, which are the correct labels before shuffling. $$ $$\n",
    "Third, we shuffle the data and the labels the same way. $$ $$\n",
    "Fourth, we split the data into training and test sets: 90 training samples, 10 test samples. $$ $$\n",
    "Fifths and sixth cells are to verify the data have the proper shapes and includes both positive and negative samples. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "int32\n"
     ]
    }
   ],
   "source": [
    "ordering=np.arange(n_samples)\n",
    "np.random.shuffle(ordering)\n",
    "print(ordering.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100,)\n"
     ]
    }
   ],
   "source": [
    "y_base=np.reshape(np.concatenate([np.ones([1,n_samples//2]),np.zeros([1,n_samples//2])],axis=1),[n_samples])\n",
    "print(y_base.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100, 727) (100,)\n"
     ]
    }
   ],
   "source": [
    "X=np.concatenate([X_coriolanus,X_octavio],axis=0)[ordering,:]\n",
    "y=y_base[ordering]\n",
    "print(X.shape,y_base.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train,X_test=X[:(9*n_samples)//10,:],X[(9*n_samples)//10:n_samples,:]\n",
    "y_train,y_test=y[:(9*n_samples)//10],y[(9*n_samples)//10:n_samples]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(90, 727) (10, 727)\n",
      "(90,) (10,)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape,X_test.shape)\n",
    "print(y_train.shape,y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 0. 1. 1. 1. 0. 1. 0. 0. 1.]\n"
     ]
    }
   ],
   "source": [
    "print(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Neural Network Implementation\n",
    "The next three cells are about setting up and training a logistic regressor implemented as a neural network. If you do not have Tensorflow, you can just skip running these 3 cells. $$ $$\n",
    "I decided to train a neural network, because initially, I did not know if a logistic regressor would suffice and wanted the flexibility to modify my model: add layers to see if I could get the model to detect a signal: a difference between the two distributions of word counts. It was not a given that the  data would be linearly separable, although it could be suspected on the basis of the sparseness of the feature vectors. $$ $$\n",
    "Neural networks with several layers can handle non-linearly separable data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Callbacks are the way to modify training without changing the model\n",
    "# on_epoch ends stops the training, in order to prevent overfitting when a given training accuracy is reached.\n",
    "class myCallback(tf.keras.callbacks.Callback):\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        if(logs.get('acc')>0.99):\n",
    "            print(\"\\nReached 99% accuracy so cancelling training!\")\n",
    "            self.model.stop_training = True\n",
    "\n",
    "callbacks = myCallback()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 80 samples, validate on 10 samples\n",
      "Epoch 1/50\n",
      "80/80 [==============================] - 1s 8ms/step - loss: 0.6820 - acc: 0.5625 - val_loss: 0.5988 - val_acc: 0.9000\n",
      "Epoch 2/50\n",
      "80/80 [==============================] - 0s 403us/step - loss: 0.6594 - acc: 0.6250 - val_loss: 0.5905 - val_acc: 0.9000\n",
      "Epoch 3/50\n",
      "80/80 [==============================] - 0s 449us/step - loss: 0.6386 - acc: 0.7500 - val_loss: 0.5832 - val_acc: 0.9000\n",
      "Epoch 4/50\n",
      "80/80 [==============================] - 0s 436us/step - loss: 0.6184 - acc: 0.7625 - val_loss: 0.5757 - val_acc: 0.9000\n",
      "Epoch 5/50\n",
      "80/80 [==============================] - 0s 436us/step - loss: 0.5988 - acc: 0.8500 - val_loss: 0.5690 - val_acc: 0.9000\n",
      "Epoch 6/50\n",
      "80/80 [==============================] - 0s 449us/step - loss: 0.5797 - acc: 0.9000 - val_loss: 0.5624 - val_acc: 0.9000\n",
      "Epoch 7/50\n",
      "80/80 [==============================] - 0s 411us/step - loss: 0.5616 - acc: 0.9125 - val_loss: 0.5556 - val_acc: 0.9000\n",
      "Epoch 8/50\n",
      "80/80 [==============================] - 0s 399us/step - loss: 0.5443 - acc: 0.9500 - val_loss: 0.5489 - val_acc: 0.9000\n",
      "Epoch 9/50\n",
      "80/80 [==============================] - 0s 399us/step - loss: 0.5272 - acc: 0.9750 - val_loss: 0.5427 - val_acc: 0.9000\n",
      "Epoch 10/50\n",
      "80/80 [==============================] - 0s 411us/step - loss: 0.5111 - acc: 0.9750 - val_loss: 0.5366 - val_acc: 0.9000\n",
      "Epoch 11/50\n",
      "80/80 [==============================] - 0s 411us/step - loss: 0.4960 - acc: 0.9750 - val_loss: 0.5306 - val_acc: 0.9000\n",
      "Epoch 12/50\n",
      "80/80 [==============================] - 0s 387us/step - loss: 0.4810 - acc: 0.9875 - val_loss: 0.5252 - val_acc: 0.9000\n",
      "Epoch 13/50\n",
      "80/80 [==============================] - 0s 436us/step - loss: 0.4665 - acc: 0.9875 - val_loss: 0.5195 - val_acc: 0.9000\n",
      "Epoch 14/50\n",
      "80/80 [==============================] - 0s 386us/step - loss: 0.4528 - acc: 1.0000 - val_loss: 0.5137 - val_acc: 0.9000\n",
      "Epoch 15/50\n",
      "80/80 [==============================] - 0s 399us/step - loss: 0.4395 - acc: 1.0000 - val_loss: 0.5084 - val_acc: 0.9000\n",
      "Epoch 16/50\n",
      "80/80 [==============================] - 0s 399us/step - loss: 0.4266 - acc: 1.0000 - val_loss: 0.5031 - val_acc: 0.9000\n",
      "Epoch 17/50\n",
      "80/80 [==============================] - 0s 399us/step - loss: 0.4144 - acc: 1.0000 - val_loss: 0.4984 - val_acc: 0.9000\n",
      "Epoch 18/50\n",
      "80/80 [==============================] - 0s 399us/step - loss: 0.4027 - acc: 1.0000 - val_loss: 0.4926 - val_acc: 0.9000\n",
      "Epoch 19/50\n",
      "80/80 [==============================] - 0s 399us/step - loss: 0.3914 - acc: 1.0000 - val_loss: 0.4883 - val_acc: 0.9000\n",
      "Epoch 20/50\n",
      "80/80 [==============================] - 0s 399us/step - loss: 0.3804 - acc: 1.0000 - val_loss: 0.4834 - val_acc: 0.9000\n",
      "Epoch 21/50\n",
      "80/80 [==============================] - 0s 399us/step - loss: 0.3699 - acc: 1.0000 - val_loss: 0.4790 - val_acc: 0.9000\n",
      "Epoch 22/50\n",
      "80/80 [==============================] - 0s 399us/step - loss: 0.3598 - acc: 1.0000 - val_loss: 0.4747 - val_acc: 0.9000\n",
      "Epoch 23/50\n",
      "80/80 [==============================] - 0s 399us/step - loss: 0.3499 - acc: 1.0000 - val_loss: 0.4705 - val_acc: 0.9000\n",
      "Epoch 24/50\n",
      "80/80 [==============================] - 0s 399us/step - loss: 0.3407 - acc: 1.0000 - val_loss: 0.4661 - val_acc: 0.9000\n",
      "Epoch 25/50\n",
      "80/80 [==============================] - 0s 399us/step - loss: 0.3315 - acc: 1.0000 - val_loss: 0.4619 - val_acc: 0.9000\n",
      "Epoch 26/50\n",
      "80/80 [==============================] - 0s 399us/step - loss: 0.3228 - acc: 1.0000 - val_loss: 0.4578 - val_acc: 0.9000\n",
      "Epoch 27/50\n",
      "80/80 [==============================] - 0s 399us/step - loss: 0.3145 - acc: 1.0000 - val_loss: 0.4540 - val_acc: 0.9000\n",
      "Epoch 28/50\n",
      "80/80 [==============================] - 0s 399us/step - loss: 0.3064 - acc: 1.0000 - val_loss: 0.4504 - val_acc: 0.9000\n",
      "Epoch 29/50\n",
      "80/80 [==============================] - 0s 374us/step - loss: 0.2986 - acc: 1.0000 - val_loss: 0.4467 - val_acc: 0.9000\n",
      "Epoch 30/50\n",
      "80/80 [==============================] - 0s 386us/step - loss: 0.2911 - acc: 1.0000 - val_loss: 0.4432 - val_acc: 0.9000\n",
      "Epoch 31/50\n",
      "80/80 [==============================] - 0s 386us/step - loss: 0.2838 - acc: 1.0000 - val_loss: 0.4396 - val_acc: 0.9000\n",
      "Epoch 32/50\n",
      "80/80 [==============================] - 0s 374us/step - loss: 0.2768 - acc: 1.0000 - val_loss: 0.4364 - val_acc: 0.9000\n",
      "Epoch 33/50\n",
      "80/80 [==============================] - 0s 386us/step - loss: 0.2701 - acc: 1.0000 - val_loss: 0.4331 - val_acc: 0.9000\n",
      "Epoch 34/50\n",
      "80/80 [==============================] - 0s 374us/step - loss: 0.2635 - acc: 1.0000 - val_loss: 0.4297 - val_acc: 0.9000\n",
      "Epoch 35/50\n",
      "80/80 [==============================] - 0s 523us/step - loss: 0.2572 - acc: 1.0000 - val_loss: 0.4262 - val_acc: 0.9000\n",
      "Epoch 36/50\n",
      "80/80 [==============================] - 0s 411us/step - loss: 0.2511 - acc: 1.0000 - val_loss: 0.4234 - val_acc: 0.9000\n",
      "Epoch 37/50\n",
      "80/80 [==============================] - 0s 411us/step - loss: 0.2453 - acc: 1.0000 - val_loss: 0.4205 - val_acc: 0.9000\n",
      "Epoch 38/50\n",
      "80/80 [==============================] - 0s 374us/step - loss: 0.2396 - acc: 1.0000 - val_loss: 0.4178 - val_acc: 0.9000\n",
      "Epoch 39/50\n",
      "80/80 [==============================] - 0s 374us/step - loss: 0.2341 - acc: 1.0000 - val_loss: 0.4148 - val_acc: 0.9000\n",
      "Epoch 40/50\n",
      "80/80 [==============================] - 0s 361us/step - loss: 0.2287 - acc: 1.0000 - val_loss: 0.4117 - val_acc: 0.9000\n",
      "Epoch 41/50\n",
      "80/80 [==============================] - 0s 349us/step - loss: 0.2235 - acc: 1.0000 - val_loss: 0.4090 - val_acc: 0.9000\n",
      "Epoch 42/50\n",
      "80/80 [==============================] - 0s 374us/step - loss: 0.2186 - acc: 1.0000 - val_loss: 0.4065 - val_acc: 0.9000\n",
      "Epoch 43/50\n",
      "80/80 [==============================] - 0s 374us/step - loss: 0.2138 - acc: 1.0000 - val_loss: 0.4041 - val_acc: 0.9000\n",
      "Epoch 44/50\n",
      "80/80 [==============================] - 0s 349us/step - loss: 0.2091 - acc: 1.0000 - val_loss: 0.4013 - val_acc: 0.9000\n",
      "Epoch 45/50\n",
      "80/80 [==============================] - 0s 399us/step - loss: 0.2046 - acc: 1.0000 - val_loss: 0.3987 - val_acc: 0.9000\n",
      "Epoch 46/50\n",
      "80/80 [==============================] - 0s 374us/step - loss: 0.2002 - acc: 1.0000 - val_loss: 0.3961 - val_acc: 0.9000\n",
      "Epoch 47/50\n",
      "80/80 [==============================] - 0s 374us/step - loss: 0.1959 - acc: 1.0000 - val_loss: 0.3935 - val_acc: 0.9000\n",
      "Epoch 48/50\n",
      "80/80 [==============================] - 0s 361us/step - loss: 0.1918 - acc: 1.0000 - val_loss: 0.3913 - val_acc: 0.9000\n",
      "Epoch 49/50\n",
      "80/80 [==============================] - 0s 374us/step - loss: 0.1878 - acc: 1.0000 - val_loss: 0.3890 - val_acc: 0.9000\n",
      "Epoch 50/50\n",
      "80/80 [==============================] - 0s 361us/step - loss: 0.1840 - acc: 1.0000 - val_loss: 0.3870 - val_acc: 0.9000\n",
      "10/10 [==============================] - 0s 199us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.4022069573402405, 1.0]"
      ]
     },
     "execution_count": 215,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Similarly to sklearn, in Keras, we specify a model: this one has one fully connected layer \"Dense\".\n",
    "# The output of the Dense layer is fed to the sigmoid function of a single unit\n",
    "model = keras.models.Sequential([keras.layers.Dense(1, activation=tf.nn.sigmoid)])\n",
    "# Keras uses the Python line above to build a graph that is compiled in C++ and will not be executed in Python\n",
    "# In building this graph, it also needs to know what optimization algorithm to use: Adam for adaptive momentum,\n",
    "# what loss function to minimize and optionally the metric we are intested in\n",
    "model.compile(optimizer = 'adam',loss = 'binary_crossentropy',metrics=['accuracy'])\n",
    "# This is the training phase:\n",
    "# No need to feed it batches and to shuffle, no need to split between training and\n",
    "# validation data sets: keras does that for you; just set hyperparameters\n",
    "# fit returns a history: an object containing a dictionary of the evolution of training:\n",
    "# training loss, validation loss, training accuracy, validation accuracy\n",
    "\n",
    "# Using the callback to stop training early is good to prevent the network for overfitting\n",
    "#history=model.fit(X_train, y_train, batch_size=10, epochs=100,callbacks=[callbacks],validation_split=1.0/9.0,shuffle=True)\n",
    "# Here the network is underfitting, so it is best to train longer.\n",
    "history=model.fit(X_train, y_train, batch_size=10, epochs=50,validation_split=1.0/9.0,shuffle=True)\n",
    "model.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 0. 0. 0. 1. 0. 1. 1. 1. 0. 1. 0. 0. 0. 1. 1. 1. 1. 0. 1. 1. 0. 0. 0.\n",
      " 1. 0. 1. 1. 0. 1. 0. 0. 1. 1. 1. 0. 1. 1. 1. 0. 1. 1. 1. 1. 0. 1. 0. 1.\n",
      " 0. 1. 1. 1. 0. 0. 0. 1. 0. 0. 1. 0. 1. 0. 0. 1. 0. 1. 1. 1. 0. 1. 0. 1.\n",
      " 0. 0. 1. 1. 0. 0. 0. 1. 1. 1. 0. 0. 0. 0. 0. 0. 1. 0.]\n"
     ]
    }
   ],
   "source": [
    "print(y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sklearn implementation\n",
    "This is the same type of logistic regressor, implemented with sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [],
   "source": [
    "LogReg = LogisticRegression(class_weight='balanced',solver='liblinear').fit(X_train,y_train.ravel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:\n",
      "1.0\n"
     ]
    }
   ],
   "source": [
    "y_pred = LogReg.predict(X_test)\n",
    "print(\"Accuracy:\")\n",
    "print(np.mean(np.float32(np.equal(y_pred,y_test))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
